import torch
import numpy as np
import pickle
import gzip
import json
from torch.utils.data import Dataset, DataLoader
import os
import time
from tqdm import tqdm


class ProteinDataset(Dataset):
    def __init__(self, data_dir, batch_size=1000, max_proteins=None, test_mode=False):
        """
        åˆå§‹åŒ–è›‹ç™½è´¨æ•°æ®é›†

        å‚æ•°:
            data_dir: è›‹ç™½è´¨æ•°æ®ç›®å½•
            batch_size: æ‰¹å¤„ç†å¤§å°
            max_proteins: æœ€å¤§è›‹ç™½è´¨æ•°é‡é™åˆ¶ (Noneè¡¨ç¤ºä¸é™åˆ¶)
            test_mode: æµ‹è¯•æ¨¡å¼ï¼ŒåªåŠ è½½å°‘é‡æ•°æ®ç”¨äºéªŒè¯
        """
        self.test_mode = test_mode
        if test_mode:
            print("âš ï¸ è¿è¡Œåœ¨æµ‹è¯•æ¨¡å¼ï¼ŒåªåŠ è½½æœ‰é™æ•°é‡çš„è›‹ç™½è´¨")
            max_proteins = max_proteins or 1000

        self.max_proteins = max_proteins
        self.data_dir = data_dir
        self.metadata = {}

        # åŠ è½½è›‹ç™½è´¨æ•°æ®
        print(f"æ­£åœ¨åŠ è½½è›‹ç™½è´¨æ•°æ®: {data_dir}")
        start_time = time.time()

        # æ£€æŸ¥æ•°æ®å­˜å‚¨æ ¼å¼
        metadata_path = os.path.join(data_dir, "protein_metadata.json")
        if os.path.exists(metadata_path):
            with open(metadata_path, 'r') as f:
                self.metadata = json.load(f)
            self.format_type = self.metadata.get("format", "standard")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°å…ƒæ•°æ®æ–‡ä»¶ï¼Œå°è¯•æ¨æ–­æ•°æ®æ ¼å¼...")
            if os.path.exists(os.path.join(data_dir, "protein_data.pkl.gz")):
                self.format_type = "compressed"
            elif os.path.exists(os.path.join(data_dir, "protein_index.pkl.gz")):
                self.format_type = "chunked_compressed"
            elif os.path.exists(os.path.join(data_dir, "protein_data.pkl")):
                self.format_type = "standard"
            else:
                raise FileNotFoundError(f"æ— æ³•åœ¨ {data_dir} æ‰¾åˆ°å¯è¯†åˆ«çš„è›‹ç™½è´¨æ•°æ®æ–‡ä»¶")

        print(f"æ•°æ®æ ¼å¼: {self.format_type}")

        # æ ¹æ®ä¸åŒå­˜å‚¨æ ¼å¼åŠ è½½æ•°æ®
        self.proteins = {}
        if self.format_type in ["chunked", "chunked_compressed"]:
            self._load_chunked_data()
        else:
            self._load_single_file()

        print(f"æ•°æ®åŠ è½½å®Œæˆï¼Œç”¨æ—¶: {time.time() - start_time:.2f}ç§’")

        # å¤„ç†é™åˆ¶
        if self.max_proteins and self.max_proteins < len(self.proteins):
            protein_ids = list(self.proteins.keys())[:self.max_proteins]
            self.proteins = {pid: self.proteins[pid] for pid in protein_ids}
            print(f"âš ï¸ å·²é™åˆ¶ä¸ºå‰ {self.max_proteins} ä¸ªè›‹ç™½è´¨")

        # æå–å¹¶å¤„ç†åºåˆ—å’Œå±æ€§
        self.protein_ids = list(self.proteins.keys())
        self.num_proteins = len(self.protein_ids)
        self.num_amino_acids = 20  # 20ç§æ ‡å‡†æ°¨åŸºé…¸

        # è®¡ç®—æ‰€æœ‰å”¯ä¸€çš„å±æ€§å
        all_props = set()
        for pid in self.protein_ids:
            props = self.proteins[pid].get('properties', {})
            if props:
                all_props.update(props.keys())

        self.num_properties = len(all_props)
        self.property_names = sorted(all_props)

        print(f"åŠ è½½äº† {self.num_proteins} ä¸ªè›‹ç™½è´¨åºåˆ—ï¼Œ{self.num_properties} ä¸ªå±æ€§")

    def _load_single_file(self):
        """åŠ è½½å•ä¸ªæ–‡ä»¶æ ¼å¼çš„æ•°æ®"""
        if self.format_type == "compressed":
            file_path = os.path.join(self.data_dir, "protein_data.pkl.gz")
            with gzip.open(file_path, 'rb') as f:
                self.proteins = pickle.load(f)
        else:
            file_path = os.path.join(self.data_dir, "protein_data.pkl")
            with open(file_path, 'rb') as f:
                self.proteins = pickle.load(f)

        print(f"ä»å•ä¸ªæ–‡ä»¶åŠ è½½äº† {len(self.proteins)} ä¸ªè›‹ç™½è´¨")

    def _load_chunked_data(self):
        """åŠ è½½åˆ†å—æ•°æ®"""
        # åŠ è½½ç´¢å¼•
        index_path = os.path.join(self.data_dir, "protein_index.pkl.gz")
        if os.path.exists(index_path):
            with gzip.open(index_path, 'rb') as f:
                self.protein_index = pickle.load(f)
        else:
            raise FileNotFoundError(f"æœªæ‰¾åˆ°ç´¢å¼•æ–‡ä»¶: {index_path}")

        # æŒ‡å®šè¦åŠ è½½çš„è›‹ç™½è´¨æ•°é‡
        all_protein_ids = list(self.protein_index.keys())
        if self.max_proteins:
            load_ids = all_protein_ids[:self.max_proteins]
            print(f"å°†åªåŠ è½½å‰ {len(load_ids)} ä¸ªè›‹ç™½è´¨")
        else:
            load_ids = all_protein_ids

        # æŒ‰å—IDç»„ç»‡è›‹ç™½è´¨
        proteins_by_chunk = {}
        for pid in load_ids:
            chunk_id = self.protein_index[pid]
            if chunk_id not in proteins_by_chunk:
                proteins_by_chunk[chunk_id] = []
            proteins_by_chunk[chunk_id].append(pid)

        # åŠ è½½æ¯ä¸ªåŒ…å«æ‰€éœ€è›‹ç™½è´¨çš„å—
        print(f"éœ€è¦åŠ è½½ {len(proteins_by_chunk)} ä¸ªæ•°æ®å—...")

        for chunk_id, chunk_proteins in tqdm(proteins_by_chunk.items(), desc="åŠ è½½æ•°æ®å—"):
            # ä¿®æ­£è¿™é‡Œ: ä½¿ç”¨æ­£ç¡®çš„æ–‡ä»¶åæ¨¡å¼
            chunk_file = os.path.join(self.data_dir, f"protein_data_chunk_{chunk_id}.pkl.gz")

            try:
                with gzip.open(chunk_file, 'rb') as f:
                    chunk_data = pickle.load(f)

                # åªæå–éœ€è¦çš„è›‹ç™½è´¨
                for pid in chunk_proteins:
                    if pid in chunk_data:
                        self.proteins[pid] = chunk_data[pid]
            except Exception as e:
                print(f"âš ï¸ è­¦å‘Š: åŠ è½½æ•°æ®å— {chunk_id} æ—¶å‡ºé”™: {str(e)}")

    def __len__(self):
        """è¿”å›æ•°æ®é›†ä¸­è›‹ç™½è´¨çš„æ•°é‡"""
        return len(self.protein_ids)

    def __getitem__(self, idx):
        """è¿”å›æŒ‡å®šç´¢å¼•çš„è›‹ç™½è´¨æ•°æ®"""
        # è·å–è›‹ç™½è´¨ID
        protein_id = self.protein_ids[idx]
        protein_data = self.proteins[protein_id]

        # è·å–åºåˆ—
        sequence = protein_data.get('sequence', '')

        # è·å–å±æ€§
        properties = protein_data.get('properties', {})

        # å°†åºåˆ—è½¬æ¢ä¸ºæ°¨åŸºé…¸ç´¢å¼•åºåˆ—
        amino_acids = "ACDEFGHIKLMNPQRSTVWY"  # 20ç§æ ‡å‡†æ°¨åŸºé…¸
        aa_indices = torch.tensor([amino_acids.find(aa) for aa in sequence if aa in amino_acids])

        # å°†å±æ€§è½¬æ¢ä¸ºç‰¹å¾å‘é‡
        property_vector = torch.zeros(len(self.property_names))

        for i, prop_name in enumerate(self.property_names):
            if prop_name in properties:
                property_vector[i] = float(properties[prop_name])

        return {
            'protein_id': protein_id,
            'sequence': aa_indices,
            'properties': property_vector,
            'raw_sequence': sequence
        }


class ProteinDataHandler:
    def __init__(self, data_dir, batch_size=32, test_mode=False, max_proteins=None):
        self.data_dir = data_dir
        self.batch_size = batch_size
        self.test_mode = test_mode

        print(f"åˆå§‹åŒ–è›‹ç™½è´¨æ•°æ®å¤„ç†å™¨ï¼Œæ‰¹å¤„ç†å¤§å°: {batch_size}")
        if test_mode:
            print("ğŸ§ª æµ‹è¯•æ¨¡å¼å·²å¯ç”¨ï¼Œå°†åªåŠ è½½å°‘é‡æ•°æ®")
            max_proteins = max_proteins or 1000

        # åŠ è½½æ•°æ®é›†
        self.dataset = ProteinDataset(
            data_dir,
            batch_size=1000,  # å†…éƒ¨æ‰¹å¤„ç†å¤§å°
            max_proteins=max_proteins,
            test_mode=test_mode
        )

        # è®¾ç½®æ•°æ®é›†å±æ€§
        self.num_proteins = self.dataset.num_proteins
        self.num_amino_acids = self.dataset.num_amino_acids
        self.num_properties = self.dataset.num_properties

        print(f"æ•°æ®é›†åˆå§‹åŒ–å®Œæˆ: {self.num_proteins}ä¸ªè›‹ç™½è´¨, {self.num_properties}ä¸ªå±æ€§")

    def test_loading(self, num_samples=5):
        """æµ‹è¯•æ•°æ®é›†åŠ è½½ï¼Œæ˜¾ç¤ºå‡ ä¸ªæ ·æœ¬"""
        print(f"\n=== æµ‹è¯•æ•°æ®é›†åŠ è½½ ({num_samples}ä¸ªæ ·æœ¬) ===")

        # å¦‚æœéœ€è¦æŸ¥çœ‹æŒ‡å®šæ•°é‡çš„æ ·æœ¬
        for i in range(min(num_samples, len(self.dataset))):
            sample = self.dataset[i]
            protein_id = sample['protein_id']
            seq_len = len(sample['sequence'])
            prop_count = (sample['properties'] != 0).sum().item()

            print(f"æ ·æœ¬ {i}:")
            print(f"  - è›‹ç™½è´¨ID: {protein_id}")
            print(f"  - åºåˆ—é•¿åº¦: {seq_len}")
            print(f"  - æœ‰å€¼å±æ€§æ•°é‡: {prop_count}")
            print(f"  - åºåˆ—å‰20ä¸ªæ°¨åŸºé…¸: {sample['raw_sequence'][:20]}...")
            print("")

        return True

    def get_dataloaders(self, valid_ratio=0.1, test_ratio=0.1):
        """åˆ›å»ºè®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®åŠ è½½å™¨"""
        dataset_size = len(self.dataset)
        print(f"åˆ’åˆ†æ•°æ®é›† (å…±{dataset_size}ä¸ªæ ·æœ¬)")
        print(f"- è®­ç»ƒé›†: {100 * (1 - valid_ratio - test_ratio):.1f}%")
        print(f"- éªŒè¯é›†: {100 * valid_ratio:.1f}%")
        print(f"- æµ‹è¯•é›†: {100 * test_ratio:.1f}%")

        indices = list(range(dataset_size))
        np.random.shuffle(indices)

        test_split = int(np.floor(test_ratio * dataset_size))
        valid_split = int(np.floor(valid_ratio * dataset_size))

        test_indices = indices[:test_split]
        valid_indices = indices[test_split:test_split + valid_split]
        train_indices = indices[test_split + valid_split:]

        # åˆ›å»ºæ•°æ®å­é›†
        from torch.utils.data import SubsetRandomSampler
        train_sampler = SubsetRandomSampler(train_indices)
        valid_sampler = SubsetRandomSampler(valid_indices)
        test_sampler = SubsetRandomSampler(test_indices)

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        print("åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        train_loader = DataLoader(
            self.dataset, batch_size=self.batch_size, sampler=train_sampler)
        valid_loader = DataLoader(
            self.dataset, batch_size=self.batch_size, sampler=valid_sampler)
        test_loader = DataLoader(
            self.dataset, batch_size=self.batch_size, sampler=test_sampler)

        print(f"æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆï¼Œè®­ç»ƒé›†: {len(train_indices)}æ ·æœ¬")

        return train_loader, valid_loader, test_loader


if __name__ == '__main__':
    import time
    import argparse

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="è›‹ç™½è´¨æ•°æ®å¤„ç†ç¨‹åº")
    parser.add_argument("--test", action="store_true", help="æµ‹è¯•æ¨¡å¼ï¼ŒåªåŠ è½½1000ä¸ªè›‹ç™½è´¨")
    parser.add_argument("--max_proteins", type=int, default=None, help="æœ€å¤§è›‹ç™½è´¨æ•°é‡")
    parser.add_argument("--batch_size", type=int, default=128, help="æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--data_dir", type=str, default="./data/protein_data",
                        help="è›‹ç™½è´¨æ•°æ®ç›®å½•")
    args = parser.parse_args()

    print("=" * 50)
    print("è›‹ç™½è´¨æ•°æ®å¤„ç†ç¨‹åº")
    print("=" * 50)
    if args.test:
        print("ğŸ§ª æµ‹è¯•æ¨¡å¼: ä»…åŠ è½½æœ‰é™æ•°é‡çš„è›‹ç™½è´¨")
    if args.max_proteins:
        print(f"âš ï¸ é™åˆ¶æœ€å¤§è›‹ç™½è´¨æ•°é‡: {args.max_proteins}")

    start_time = time.time()

    try:
        # åˆ›å»ºæ•°æ®å¤„ç†å™¨
        handler = ProteinDataHandler(
            data_dir=args.data_dir,
            batch_size=args.batch_size,
            test_mode=args.test,
            max_proteins=args.max_proteins
        )

        # æµ‹è¯•æ•°æ®åŠ è½½
        handler.test_loading(num_samples=3)

        # è·å–æ•°æ®åŠ è½½å™¨
        train_loader, valid_loader, test_loader = handler.get_dataloaders()

        # æµ‹è¯•åŠ è½½ä¸€ä¸ªæ‰¹æ¬¡
        print("\næµ‹è¯•åŠ è½½ä¸€ä¸ªæ‰¹æ¬¡...")
        batch = next(iter(train_loader))
        print(f"æ‰¹æ¬¡å¤§å°: {len(batch['protein_id'])}")
        print(f"æ ·æœ¬0çš„è›‹ç™½è´¨ID: {batch['protein_id'][0]}")
        print(f"æ ·æœ¬0çš„åºåˆ—é•¿åº¦: {len(batch['sequence'][0])}")

        total_time = time.time() - start_time
        print(f"\nç¨‹åºæ‰§è¡Œå®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")

    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯: {e}")
        import traceback

        traceback.print_exc()